# components of Azure Cosmos DB for NoSQL?
* <ins>Accounts</ins> - fundamental units of distribution and high availability. At the account level, configure the region[s] for your data & set the default consistency level for requests. Accounts also contain the globally unique DNS name used for API requests. 
* <ins>Databases</ins> -  Each account can contain one or more Databases. A database is a logical unit of management for containers
* <ins>Containers</ins> - fundamental unit of scalability. At the container level, provision throughput and Optionally configure an indexing policy or a default time-to-live value. Cosmos will automatically and transparently partition the data in a container.
* <ins>Items</ins> - The NoSQL API for stores individual documents in JSON format as items within the container. Cosmos can provide fast and predictable performance because write operations on JSON documents are atomic.
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/7a66330a-d3b1-4042-bb1c-755f09a092a9)
* Partitioning & Partition Keys
    1. Partitioning involves writing data to servers in a way that optimizes both reads and writes.
    2. Cosmos DB stores data in virtual buckets called logical partitions.
    3. It relies on a partition key to determine which of these buckets to put new data in and where to look for data during a query. 
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/02be67b0-4a30-4308-abfc-3cf843ade80a)
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/6342c9b4-1171-401e-b902-3681287ab8be)
    4. When choosing a key, following three guidelines.
        1. Find the write balance - Test your partition key to see how it distributes writes. Avoid hotspots and rate limits by achieving even distribution of storage and throughput across logical partitions.
        2. Aim for a single partition query. Look to see how many partitions get hit when you run your most frequent queries. Aoid the cost and latency of involving multiple partitions, by choosing a key that queries a single partition.
        * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/601d6081-3aed-4428-98e7-0095c5ab2113)
        * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/1bc3fe85-b71b-4cc1-8f69-d41ee1e41404)
        3. Understand cross-partition query trade offs. If you do run cross-partition queries for less important workloads every once in a while, it won't impact your overall experience. But if it's more than that, you can use an array of discrete values for the partition keys in your query to target a subset of partitions.
        * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/d3030a58-1b10-4d20-97a3-fcdf76fcaeb4)
        * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/e17ab35d-a944-49b2-98d6-39c81642c3b4)
    5. Every container is required to specify a partition key path. Behind the scenes, Cosmos uses this path to logically partition data using partition key values. For example, consider the following JSON document:
    ``` mark
        {
          "id": "35b5bf7d-5f0e-4209-b7cb-8c5c70c3bb59",
          "deviceDisplayName": "shared-printer",
          "acquiredYear": 2019,
          "department": {
            "name": "information-technology",
            "metadata": {
              "location": "floor-5-unit-27"
            }
          },
          "queuedDocuments": [
            {
              "sender": "user-293749329",
              "sentTime": "2019-07-26T05:12:37",
              "pages": 5,
              "spoolRef": "3f4b759c-3230-4269-a88e-de7620ad91c0"
            },
            {
              "device": {
                "type": "mobile"
              },
              "sentTime": "2019-11-12T13:08:42",
              "spoolRefs": [
                "6a86682c-be5a-4a4a-bacd-96c4d1c7ece6",
                "79e78fe2-93aa-4688-89db-a7278b034aa6"
              ]
            }
          ]
        }
    ```
        1. container specifies a partition key path of /department/name, then the partition key value of this document would be information-technology. Behind the scenes, CosmosL automatically manages the physical resources necessary to support your data workload.

# Understand throughput
* Each container is a unit of scalability for both throughput and storage.
* Containers are partitioned horizontally across compute within an region and distributed across all Regions you configure in your Cosmos account.
* Container-level throughput provisioning
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/06b4c921-2392-4678-86a4-a5d2348fb698)
* Database-level throughput provisioning
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/725fe5be-97b2-4712-9cea-821224df24f7)
* Mixed-throughput provisioning
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/589be45c-f850-459c-a17a-1bd45fd460b5)

# Evaluate throughput requirements
* Request unit(RU)s are a rate-based currency. They are used to make it simple to talk about physical resources like memory, CPU, and IO when performing requests in Cosmos. 
* RUs are used to measure both foreground and background activities.
* Every request consumes a fixed number of RUs, including but not limited to: Reads, Writes, Queries, Stored procedures
* Configuring throughput - create a db or container in CosmosDB, you can provision RUs in an increment of request units per second (or RU/s for short). You cannot provision less than 400 RU/s, and they are provisioned in increments of 100.
* Estimating ad-hoc RU/s consumption - For example, estimate the RU/s required for common db operations such as one RU for a read and six RU/s for a write operation of a 1-KB document in optimal conditions.
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/2ebf2817-7e39-4476-92c8-08b708ddeec5)
* Build a quick table to figure out a rough estimate of your needed RU capacity. Like:
Operation type|Number of requests per second|Number of RU per request|RU/s needed
    Write Single Document|10,000|10|100,000
    -|-|-|-
    Top Query #1|700|100|70,000
    Top Query #2|200|100|20,000
    Top Query #3|100|100|10,000
    Total RU/s|||200,000 RU/s

# Evaluate data storage requirements
* Migrating existing transactional workloads - Use CosmosDB Capacity Calculator to help estimate your app's storage and throughput requirements and translate it to a cost estimate in terms of CosmosDB.
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/2ce0ab5e-b750-4719-aa49-e19f025b103e)

# Time-to-live (TTL)
* Set the length of time documents live in the db before being automatically purged. It's measured in seconds from the last modification and can be set at the container level with the ability to override on a per-item basis.
* The maximum TTL value is 2147483647.
* Configuring TTL on a container using the <ins>DefaultTimeToLive</ins> property: Does not exist(not automatically expired), -1(not expire by default), n(n seconds after last modified time).
* Examples
    Container.DefaultTimeToLive|Item.ttl|Expiration in seconds
    -|-|-
    1000|null|1000
    1000|-1|This item will never expire
    1000|2000|2000
    Container.DefaultTimeToLive|Item.ttl|Expiration in seconds
    null|null|This item will never expire
    null|-1|This item will never expire
    null|2000|TTL is disabled at the container level. This item will never expire.

# Move data into and out of Azure Cosmos DB for NoSQL
* <ins>Azure Data Factory</ins> is a native service to ETL across sinks and stores in an entirely serverless fashion.
* <ins>Apache Kafka</ins> used to stream events in a distributed manner. Kafka Connect is a tool within their suite to stream data between Kafka and other data systems.
* <ins>Azure Stream Analytics</ins> is a real-time event-processing engine designed to process fast streaming data from multiple sources simultaneously.
* <ins>With Azure Synapse Analytics and Azure Synapse Link for Azure Cosmos DB</ins>, you can create a cloud-native hybrid transactional and analytical processing (HTAP) to run analytics over your data in Azure Cosmos DB for NoSQL. This connection enables integration over your data pipeline on both ends of your data world, Azure Cosmos DB and Azure Synapse Analytics.

# Enable offline development
* <ins>The Azure Cosmos DB emulator</ins is a great tool for common Dev+Test workflows that developers may need to implement on their local machine.
    
* Handle connection errors - Built-in retry    
    
# Implement threading and parallelism
* While the SDK implements thread-safe types and some degrees of parallelism, there are best practices that you can implement in your application code to ensure that the SDK has the best performance it can possibly have in your workload.
* Avoid resource-related timeouts - occur due to high CPU or port utilization on client machines rather than a service-side issue. So scale-out.
* Use async/await in .NET -  series of Task-based features to asynchronously invoke SDK client methods. 
* Use built-in iterators instead of LINQ methods - LINQ methods such as ToList will eagerly and synchronously drain a query while blocking any other calls from executing. 
    1. For example, this invocation of ToList() will block all other calls and potentially retrieve a large set of data: container.GetItemLinqQueryable<T>().Where(i => i.categoryId == 2)   .ToList<T>();
    2. The SDK includes methods such as ToFeedIterator<T> that asynchronously retrieves the results of a query without blocking other calls. container.GetItemLinqQueryable<T>().Where(i => i.categoryId == 2).ToFeedIterator<T>();
    3. Configure max concurrency, parallelism, and buffered item count - When issuing a query from the SDK, the QueryRequestOptions includes a set of properties to tune a query's performance.
        1. Max item count - All query results are returned as "pages" of results. the number of items you would like to return in each "page". Default is 100. set -1 to set a dynamic page size.
        2. Max concurrency - the number of concurrent operations ran client side during parallel query execution. If set to 1, parallelism is effectively disabled. If set to -1, the SDK manages this setting. Ideally, you would set this value to the number of physical partitions for your container.
        3. Max buffered item count - the maximum number of items that are buffered client-side during a parallel query execution. If set to -1, the SDK manages this setting. The ideal value for this setting will largely depend on the characteristics of your client machine.

# transactions in the context of JavaScript SDK  
* Stored procedures are scoped to a single logical partition. You cannot execute a stored procedure that performs operations across logical partition key values.
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/e5f77814-9ed5-4daf-98d9-28933476e019)
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/f39966c5-3ef8-434d-9cad-ac0af2febb98)
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/ba77777a-f205-4e91-a526-c1206978ae3c)

# Create user-defined functions    
* UDFs can only be called from inside queries as they enhance and extend the SQL query language.    
* ``` mark
    A simple SQL query: 
        SELECT p.name, p.price FROM products p   
    return
        { "name": "Black Bib Shorts (Small)", "price": 80.00 }    
  ```  
* ``` 
    updated query with the udf function "addTax": 
        SELECT p.name, p.price, udf.addTax(p.price) AS priceWithTax FROM products p  
    return:
          {
            "name": "Black Bib Shorts (Small)",
            "price": 80.00,
            "priceWithTax": 92.00
          }
    
    function addTax(preTax) {
        return preTax * 1.15;
    }    
  ```  
    
# Add triggers to an operation    
* Triggers can inject business logic both before and after operations. 
* Triggers are resources stored within a container, and their code is written in JavaScript, much like stored procedures and user-defined functions.   
* Pre-trigger Example - before an operation and cannot have any input parameters. Adding label, if not exist
* ``` mark    
    {
      "id": "caab0e5e-c037-48a4-a760-140497d19452",
      "name": "Handlebar",
      "categoryId": "e89a34d2-47ee-4da8-bcf6-10f552604b79",
      "categoryName": "Accessories",
      "price": 50
    } 
    
    function addLabel() {
        var context = getContext();
        var request = context.getRequest();

        var pendingItem = request.getBody();

        if (!('label' in pendingItem))
            pendingItem['label'] = 'new';

        request.setBody(pendingItem);
    }
    
    {
      "id": "caab0e5e-c037-48a4-a760-140497d19452",
      "name": "Handlebar",
      "categoryId": "e89a34d2-47ee-4da8-bcf6-10f552604b79",
      "categoryName": "Accessories",
      "price": 50,
      "label": "new"
    }
  ```
* Post-trigger example-  run after an operation has completed and can have input parameters even though they are not required.  create a second item with a different materialized view of our data
* ``` mark
    {
      "sourceId": "caab0e5e-c037-48a4-a760-140497d19452",
      "categoryId": "e89a34d2-47ee-4da8-bcf6-10f552604b79",
      "displayName": "Handlebar [Accessories]",
    }
    
    function createView() {
        var context = getContext();
        var container = context.getCollection();
        var response = context.getResponse();

        var createdItem = response.getBody();

        var viewItem = {
            sourceId: createdItem.id,
            categoryId: createdItem.categoryId,
            displayName: `${createdItem.name} [${createdItem.categoryName}]`
        };

        var accepted = container.createDocument(
            container.getSelfLink(),
            viewItem,
            (error, newItem) => {
                if (error) throw error;
            }
        );
        if (!accepted) return;
    }
  ```  

# Understand Always Encrypted   
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/058badc6-c56d-421b-8133-990cf4231993)

# Azure Monitor  
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/9ee2cd71-97e9-4aea-be63-8f55cdf2e2f1)
* Cosmos DB monitors its server-side counters using:
    1. Azure Monitor to monitor metrics: Collects Cosmos DB metrics by default. Metrics are collected every minute. The default retention period is 30 days. 
    2. Azure Monitor to monitor diagnostic logs: Telemetries like events and traces are stored as logs. 
    3. The Azure Cosmos DB portal:  The default retention period for these metrics is seven days.
    4. The Cosmos DB NoSQL API SDKs to programmatically monitor the account
    
* Measure throughput
    * Azure Monitor provides the Total RUs metric that can be used to analyze the RUs consumed by the different CosmosDB operations. This metric can then be used to analyze those operations with the highest throughput.
    * Monitoring this metric allows us to:
        1. Identify operations that are consuming more RUs than others.
        2. Identify operations that are taking more cumulative RUs in a given interval of time.
    * By identifying the operations with higher throughput, we can for example:
        1. Determine if these operations are insert and upserts, their index definition can be reviewed for over or under indexing-specific fields. We can then determine if we should include or exclude paths in their indexing policy.
        2. Modify the query to use and index with a filter clause.
        3. Use partition keys that will minimize the fan out of a query into different partitions.
        4. If possible, evaluate if a smaller result set would meet the query needs.
    * View the Total Request Unit metrics
    * To view the Total Request Units metric, under Azure Monitor's Metrics:
        1. Select the Resource Type CosmosDB accounts and Apply in the scope dialog.
        2. Select the correct CosmosDB account from the drop-down list.
        3. Under Metrics, select Total Request Units and the type of aggregation you need.
        4. If needed, refine the Time range and Time granularity of the metric.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/56a6a376-9281-44a4-861b-d3821043f88e)
    * Filter the Total Request Units further - To better analyze the throughput, Using the Add filter and Apply splitting options will help us with those analyses.
    * Azure Monitor allows us to filter further by specific CollectionName, DatabaseName, OperationType, Region, Status, and StatusCode. 
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/96e32dfa-bc18-4eed-b139-fc56bc4d5026)
    
# Observe rate-limiting events
* 429 status code error, a Request rate too large exception
* 3 main reasons why we get a 429 exception:
    1. Request rate is large.
    2. The request did not complete due to a high rate of metadata requests.
    3. The request did not complete due to a transient service error.
* Request rate is large, investigate - Cosmos DB account -> Under Insights->Request menu ->Total Request by Status Code charts -> search for occurrences of the 429 exception -> If needed, filter the charts by Time Range and Database
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/fc1d36ea-c2da-46e8-a96b-4e1c22172835)
    * the % of 429 exceptions > 5%, it's possible that the exceptions are caused by a hot partition.
    * To verify if the database access is coming across a hot partition -> under Insights->Throughput, review the Normalized RU Consumption (%) By PartitionKeyRangeID charts.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/a6157e41-9549-45f7-954d-39ee3830133e)
* Rate limiting on metadata requests - a high volume of the following metadata operations:
    1. Create, read, update, or delete a container or database
    2. List databases or containers in a Cosmos account
    3. Query the current provisioned throughput
* investigate -> under Insights->System, review the Metadata Requests That Exceeded Capacity (429s) charts.
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/62ae3fb0-32e0-4ced-bb27-0ef6d8eee7c2)
* Rate limiting due to transient service error (503) - reported in the Insights->Request Total Request by Status Code charts. These exceptions could indicate that the 429 exceptions are happening because of transient service errors.    
    
# Index usage
* The query engine evaluates query filters and then traverses the index of your container. 
* The query engine will automatically try to use the 3 most efficient:
*   Method|Description|RU implication
    |-|-|-
    Index seek|seek an exact match on a field’s value.|The RU charge is constant for the lookup. The RU charge for loading and returning items is linear based on the number of items.
    Index scan|find all possible values for a field.|The RU charge is still constant for the lookup, with a slight increase over the index seek based on the cardinality of the indexed properties. The RU charge for loading and returning items is still linear based on the number of items returned.
    Full scan|load the items, in their entirety, to the transactional store to evaluate the filters.|This type of scan does not use the index; however, the RU charge for loading items is based on the number of items in the entire container.
* Example
    ``` mark
            [
              {
                "id": "1",
                "name": "Touring-1000 Blue",
                "price": 675.55
              },
              {
                "id": "2",
                "name": "Mountain-400-W Silver",
                "price": 1215.40
              },
              {
                "id": "3",
                "name": "Road-200 Red",
                "price": 405.85
              }
            ]
    ```
* Each of these items could be visualized as a tree -same like 3rd also
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/b781736b-5f6b-42d7-8d77-2bb882a28318)
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/89895ce8-0cbf-474e-80fe-d38bab070023)    
* <ins>An inverted tree</ins> that includes all three items would have a root node that matches all three items. 
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/3dd021aa-33a9-4add-9889-ce78f97e0a7c)
* To traverse the tree, a SQL query: SELECT * FROM products p WHERE p.name = 'Touring-1000 Blue'
* The query engine could then traverse the tree in the following order:
    1. The engine will start at the root (all items are still potential matches).
    2. traverse the name node. Still, all items match.
    3. traverse the exact match, Only item #2 matches at this point
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/ac5d22e0-0114-4f55-a5b8-e2b0199c8f5a)
* This traversal is an example of the <ins>index seek</ins> lookup method in action.
    * The matching of an exact value is a <ins>flat charge in RU/s</ins> since it uses the index instead of searching in each item’s. If no match, no items will be returned in the result set. If multiple items, the query engine to return multiple items.
* Another example of an index seek: SELECT * FROM products p WHERE p.name IN ('Road-200 Red', 'Mountain-400-W Silver')
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/3dfaaf4d-1930-43f6-be31-6dba13a92d13)
*  complex index lookup method: SELECT * FROM products p WHERE p.price >= 500 AND p.price <= 1000
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/237b5836-f699-4d25-abc1-52b4a1ff5ec9)
                                                                                                
# Choose a partition key
* JSON Data documents stored in CosmosDB within containers, routed to the appropriate physical partition based on the value of a partition key.
* The partition key is a required document property. A physical partition supports a fixed maximum amount of storage and throughput (RU/s). 
* CosmosDB automatically distributes the logical partitions across the available physical partitions, again using the partition key.
* CosmosDB increase storage and throughput by adding more physical partitions to access and store data. 
* The maximum storage size of a physical partition is 50 GB, and the maximum throughput is 10,000 RU/s.
* Logical partitions in Azure Cosmos DB - the grouping together of documents with the same partition key value. 
    1. Multiple logical partitions can be stored within a single physical partition and the container can have an unlimited number of logical partitions. 
    2. Individual logical partitions are moved to new physical partitions as a unit as the container grows. 
    3. Moving logical partitions as a unit ensures that all documents within it reside on the same physical partition. 
    4. The maximum size for a logical partition is 20 GB
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/3db36bc6-97cd-4f1f-9955-ded6da298765)
    5. A container is for all data stored with the same partition key. The partition key is defined when you create a container. For example, the container has a partition key of /username.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/52f82565-65ff-4e7f-aeef-fa00b31c69fe)
* Avoid hot partitions
    1. If not test in dev env, a poor choice for partition key might not be revealed until the application is in production and significant data has already been written.
    2. When data is not partitioned correctly, it can result in hot partitions. Hot partitions prevent your application workload from scaling, and they can occur on both storage and throughput.
    3. A hot partition on storage occurs when you have a partition key that results in highly asymmetric storage patterns. As an example, consider a multitenant application that uses TenantId as its partition key with five tenants: A to F. Tenants B,C,D and E are very small, Tenant D has a little more data. However Tenant A is massive and quickly hits the 20-GB limit for its partition. In this scenario, we need to select a different partition key that will spread the storage across more logical partitions.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/a1eeee66-d390-4c34-aa9d-3ed64965bc0e)
    4. Throughput hot partitions - As an example, if you have a container with 30,000 RU/s, this workload is spread across the three physical partitions for the same six tenants mentioned earlier. So each physical partition gets 10,000 RU/s. If tenant D consumes all of its 10,000 RU/s, it will be rate limited because it can't consume the throughput allocated to the other partitions. This results in poor performance for tenant C and D, and leaving unused compute capacity in the other physical partitions and remaining tenants. Ultimately, this partition key results in a database design where the application workload can't scale.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/ad0b7d9d-11bb-48a0-ba04-88af279b00fa)
    5. When data and requests are spread evenly, the database can grow in a way that fully utilizes both the storage and throughput. The result will be the best possible performance and highest efficiency. In short, the database design will scale.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/0b201e32-3f32-4edf-b774-056c75fe7f1d)
    6. Consider reads versus writes - When you're choosing a partition key, need to consider whether the data is read heavy or write heavy. You should seek to distribute write-heavy requests with a partition key that has high cardinality.
    7. For read-heavy workloads, you should ensure that queries are processed by one or a limited number of physical partitions by including an WHERE clause with an equality filter on the partition key, or an IN operator on a subset of partition key values in your queries.
    8. in scenarios where the application workload is both write heavy and read heavy. The following illustration shows a container that's partitioned by username. This query will hit only a single logical partition, so its performance will always be good.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/562ec8a6-6d11-4d87-8c12-e0119f55c54e)
    9. A query that filters on a different property, such as favoriteColor, would "fan out" to all partitions in the container. This is also known as a <ins>cross-partition query</ins>. Such a query will perform as expected when the container is small and occupies only a single partition. However, as the container grows and there are increasing number of physical partitions, this query will become slower and more expensive because it will need to check every partition to get the results whether the physical partition contains data related to the query or not.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/1574d531-0f66-447d-85e8-125d22be38a1)
    10. Perform 3 operations on customers: create, update and retrieve. In this case, we'll retrieve the customer by their id. Because that operation will be called the most, it makes sense to make the customer's ID the partition key for the container.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/484fd93f-98aa-48d5-81b1-7745ddabeac2)
    11. making the ID the partition key means that we'll have as many logical partitions as there are customers, with each logical partition containing only a single document. Millions of customers would result in millions of logical partitions.
    12. But this is perfectly fine! Logical partitions are a virtual concept, and there's no limit to how many logical partitions you can have. CosmosDB will collocate multiple logical partitions on the same physical partition. As logical partitions grow in number or in size, Cosmos DB will move them to new physical partitions when needed.

# Model small lookup entities
* ProductCategory and ProductTag - reference values and are related to other entities though a 1:Many relationship.
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/9674ba32-27d9-42e9-9fb5-0b7a0d2b0f5c)
* Model product categories - put it in a new container called ProductCategory.
    1. Next choose a partition key. Opeations : new, edit (not frequent) and list all (often when customer visit) product category. 
    2. list all query: SELECT * FROM c. With id as the selected partition key this query will now be cross-partition, try to optimize read-heavy operations use only a single partition if possible. the  data for product category will never grow near 20 GB in size, so how would the data in a way that will result in a single partition query when we list all product categories.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/55bc5daf-c292-4803-bd3f-27ec0b4efa1a)
    4. add an entity discriminator property a constant value to our schema and use this as the partition key for this container. In this case, query: SELECT * FROM c WHERE c.type = ”category”.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/458cdf25-ef85-4af0-a330-58da35ec4682)
* Model product tags - nearly identical in function to ProductCategory. 
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/804909d5-238c-40a4-a97c-26e7442ec52f)
    
# Denormalize data in your model
* the product table from your relational database and model it for a NoSQL database. the many-to-many relationship your product table has with product tags.
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/8597837a-377a-4e2a-a93e-b7667ea274bb)
* Model the product entities
    1. application must display the product category name when you display a product.
    2. query for products by product tags within a product category as well. 
    3. 2 ways: store products in a product tags container, or embed your tags in the product container. 
    4. fewer tags per product than products per tag, so embed the product tags in the product table. There is a one-to-few relationship between each product and the tags, which makes a good case for embedding. 
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/3df12a8a-b5b3-496e-8153-7d8cc9a2dffe)
* Select a partition key - the operations (crete or edit) to be performed to decide on a partition key. 
    1. select a partition key for the product container. 
    2. As customers navigate by product category (filters products by categoryId)
    3. a single-partition query with all products by category.
    4. categoryId is a good partition key, retrieve all products in a category efficiently
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/0f437f9d-3dc9-4553-8fa9-641a5f6603aa)
    5. Currently, to display a product page for a category: Query the product container, the productCategory (name) container, using the first query, run a 3rd query on the productTag container to get each product tag name.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/005e8a25-8e2f-4b54-a8a6-36a9e0792fd5)

* Denormalize product entities
    1. Running all the preceding queries could work for you (not scalable). 
    2. in NoSQL, no joins between containers. 
    3. For NoSQL, reduce requests number can fetch data in as few requests as possible.
    4. The solution, is to denormalize data. Add more properties (category name and each tag in your tags array. Now retrieve all the data in a single request.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/de4f5e73-87fd-4159-852c-fd5ef8d8010b)

* Manage referential integrity by using change feed
    1. denormalizing data can drastically improve performance and lower RU cost by providing the data needed in a single request. 
    2. when data is denormalized like this, maintain <ins>referential integrity</ins> between the master data in the productCategory and productTag containers, and the product container.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/6e2c8684-20ff-4fed-932c-67a64bc8af84)
    3. CosmosDB <ins>change feed</ins> API that can manage referential integrity. Lives within every CosmosDB container. 
    4. insert or update, change feed streams these changes to an API that you can listen to. 
    5. When an event is triggered, you can use change feed to execute code that responds to the changed data.
    6. use change feed to listen to the productCategory, productTag and propagate changes to the product container.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/394b6964-141b-4cb2-9b91-6bbdc132aa74)
    
# Identify access patterns for your app
* Designing a data model for a NoSQL database, the objective is to ensure that operations on data are done in the fewest requests. 
* understand the relationships between the data and how data will be accessed by the application. 
* These access patterns, will determine how the properties of the various entities are grouped together and stored in documents within containers in CosmosDB for NoSQL databases.
* Identify access patterns for customer entities - 3 entities and the relationships between them.
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/eeeb1c31-bfd1-4937-9099-063b665c7d62)
* 3 customer operations: Create (first visit), Update (profile), Retrieve (sign password & access).
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/f381e8d4-492a-4124-9c88-69574eb831f9)like.

# When to embed or reference data
* embedded the customer address and password data into a new customer document, reduces the number of requests, which improves performance and reduces cost. 
* When should you embed data?
    1. Read or updated together: Data that's read or updated together is nearly always modeled as a single document. In our scenario, all of the customer entities are read or written together.
    2. 1:1 relationship: For example, Customer and CustomerPassword have a 1:1 relationship.
    3. 1:Few relationship: In a NoSQL database, it's necessary to distinguish 1:Many relationships as bounded or unbounded. Customer and CustomerAddress have a bounded 1:Many relationship.
* When should you reference data?
    1. Read or updated independently:  where combining entities that would result in large documents. Updates in CosmosDB require the entire item to be replaced. If a document has a few properties that are frequently updated alongside a large number of mostly static properties, it's much more efficient to split the document into two. One document then contains the smaller set of properties that are updated frequently. The other document contains the static, unchanging values.
    2. 1:Many relationship: if the relationship is unbounded. CosmoDB has a maximum document size of 2 MB. So in situations where the 1:Many relationship is unbounded or can grow extremely large, data should be referenced, not embedded.
    3. Many:Many relationship: example of product tags.

# Combine multiple entities in the same container
* Model sales order entities
    1. sales order detail makes a great candidate for embedding. This is because the items in the order are not unbounded and the data is always inserted and read together. So you'll embed sales order details as an array within your sales order entity. And you'll store your data in a new container, called salesOrder.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/3a9ecf70-0641-44ab-985b-821b7f4abb95)
    2. Choose partition key - always search for sales order by customer, customerId.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/cd5f6fca-c88d-4eb6-a8b7-c02af200e2c1)
     
* Identify optimization opportunities
    1. the salesOrder container shares the same partition key as the customer container.
    2. The customer container has a partition key of ID and salesOrder has a partition key of customerId. 
    3. When data share a partition key and have similar access patterns, they're candidates for being stored in the same container. 
    4. As a NoSQL database, CosmosDB is schema agnostic, so mixing entities with different schema is not only possible but, under these conditions, it's also another best practice. But to combine the data from these two containers, you'll need to make more changes to your schema.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/4a7a7528-962a-416a-a682-6ee8e3cf26a7)
    5. diagram showing the sales orders and customer models and containers going to a single customer container with both the sales order and customer documents stored in it.
    6. add a customerId property to each customer document. Customers will now have the same value for ID and customerId. 
    7. distinguish a sales order from a customer in the container.add a type property, value of customer and salesOrder for each entity.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/07ca81a8-1c66-44ac-8486-96216171ec3a)
    8. now store both the customer data and sales order data in your new customer container. Each customer is in its own logical partition and will have one customer item with all its sales orders. For your second operation here, you now have a query you can run to list all sales orders for a customer.

* Denormalize aggregates in the same container
    1. last operation - query top 10 customers by the number of sales orders. 
    2. In your current model, you first do a group by on each customer and sum for sales orders in your customer container. You then sort in descending order and take the top 10 results. 
    3. Even though customers and sales orders sit in the same container, this type of query is not something you can currently do.
    4. The solution here is to denormalize the aggregate value in a new property, salesOrderCount, in the customer document. 
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/deb562bf-6bf8-47a1-ab3a-bbe70fa422c0)
    5 Now, every time a customer creates a new sales order and a new sales order is inserted into your customer container, you need a way to update the customer document and increment the salesOrderCount property by one. 
    6. To do this by a transaction,  when the data sits within the same logical partition.
    7. Because customers and sales orders reside in the same logical partition, you can insert the new sales order and update the customer document within a transaction. 
    8. implement transactions: by using SPs or a feature called transactional batch
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/cfd73d31-7515-40f7-b889-1a342b781a17)                                                             
* Finalize the data model
    1. transformed nine relational database tables into four containers for your NoSQL database. 
    2. customer container contains customer and sales order data. 
    3. The product container contains products and many-to-many product tags. 
    4. the other two are the productTag and productCategory containers.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/d892cc83-b22e-455b-9978-b38d347996b6)
    5. One final optimization - the productCategory and productTag containers share the same partition key? because they share this key, put both entities into the same container and give it a more generic name, such as productMeta.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/10371eac-991a-43b0-874e-b1295b5e9e99)
    6. queries to get all your product tags and product categories. This pattern works for any kind of master or reference data you need to maintain. 
    7. Because all of this data is in the same container, you can also use just a single host for Change Feed to maintain referential integrity across the entire database, rather than one for each individual container. 
    8. change feed for any entity can be routed by the type property when the new data is read by change feed.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/6c25391d-bd35-41aa-81a7-87e2a62c3c32)
