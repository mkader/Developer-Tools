# components of Azure Cosmos DB for NoSQL?
* <ins>Accounts</ins> - fundamental units of distribution and high availability. At the account level, configure the region[s] for your data & set the default consistency level for requests. Accounts also contain the globally unique DNS name used for API requests. 
* <ins>Databases</ins> -  Each account can contain one or more Databases. A database is a logical unit of management for containers
* <ins>Containers</ins> - fundamental unit of scalability. At the container level, provision throughput and Optionally configure an indexing policy or a default time-to-live value. Cosmos will automatically and transparently partition the data in a container.
* <ins>Items</ins> - The NoSQL API for stores individual documents in JSON format as items within the container. Cosmos can provide fast and predictable performance because write operations on JSON documents are atomic.
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/7a66330a-d3b1-4042-bb1c-755f09a092a9)
* Partitioning & Partition Keys
    1. Partitioning involves writing data to servers in a way that optimizes both reads and writes.
    2. Cosmos DB stores data in virtual buckets called logical partitions.
    3. It relies on a partition key to determine which of these buckets to put new data in and where to look for data during a query. 
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/02be67b0-4a30-4308-abfc-3cf843ade80a)
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/6342c9b4-1171-401e-b902-3681287ab8be)
    4. When choosing a key, following three guidelines.
        1. Find the write balance - Test your partition key to see how it distributes writes. Avoid hotspots and rate limits by achieving even distribution of storage and throughput across logical partitions.
        2. Aim for a single partition query. Look to see how many partitions get hit when you run your most frequent queries. Aoid the cost and latency of involving multiple partitions, by choosing a key that queries a single partition.
        * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/601d6081-3aed-4428-98e7-0095c5ab2113)
        * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/1bc3fe85-b71b-4cc1-8f69-d41ee1e41404)
        3. Understand cross-partition query trade offs. If you do run cross-partition queries for less important workloads every once in a while, it won't impact your overall experience. But if it's more than that, you can use an array of discrete values for the partition keys in your query to target a subset of partitions.
        * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/d3030a58-1b10-4d20-97a3-fcdf76fcaeb4)
        * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/e17ab35d-a944-49b2-98d6-39c81642c3b4)
    5. Every container is required to specify a partition key path. Behind the scenes, Cosmos uses this path to logically partition data using partition key values. For example, consider the following JSON document:
    ``` mark
        {
          "id": "35b5bf7d-5f0e-4209-b7cb-8c5c70c3bb59",
          "deviceDisplayName": "shared-printer",
          "acquiredYear": 2019,
          "department": {
            "name": "information-technology",
            "metadata": {
              "location": "floor-5-unit-27"
            }
          },
          "queuedDocuments": [
            {
              "sender": "user-293749329",
              "sentTime": "2019-07-26T05:12:37",
              "pages": 5,
              "spoolRef": "3f4b759c-3230-4269-a88e-de7620ad91c0"
            },
            {
              "device": {
                "type": "mobile"
              },
              "sentTime": "2019-11-12T13:08:42",
              "spoolRefs": [
                "6a86682c-be5a-4a4a-bacd-96c4d1c7ece6",
                "79e78fe2-93aa-4688-89db-a7278b034aa6"
              ]
            }
          ]
        }
    ```
        1. container specifies a partition key path of /department/name, then the partition key value of this document would be information-technology. Behind the scenes, CosmosL automatically manages the physical resources necessary to support your data workload.

# Understand throughput
* Each container is a unit of scalability for both throughput and storage.
* Containers are partitioned horizontally across compute within an region and distributed across all Regions you configure in your Cosmos account.
* Container-level throughput provisioning
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/06b4c921-2392-4678-86a4-a5d2348fb698)
* Database-level throughput provisioning
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/725fe5be-97b2-4712-9cea-821224df24f7)
* Mixed-throughput provisioning
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/589be45c-f850-459c-a17a-1bd45fd460b5)

# Evaluate throughput requirements
* Request unit(RU)s are a rate-based currency. They are used to make it simple to talk about physical resources like memory, CPU, and IO when performing requests in Cosmos. 
* RUs are used to measure both foreground and background activities.
* Every request consumes a fixed number of RUs, including but not limited to: Reads, Writes, Queries, Stored procedures
* Configuring throughput - create a db or container in CosmosDB, you can provision RUs in an increment of request units per second (or RU/s for short). You cannot provision less than 400 RU/s, and they are provisioned in increments of 100.
* Estimating ad-hoc RU/s consumption - For example, estimate the RU/s required for common db operations such as one RU for a read and six RU/s for a write operation of a 1-KB document in optimal conditions.
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/2ebf2817-7e39-4476-92c8-08b708ddeec5)
* Build a quick table to figure out a rough estimate of your needed RU capacity. Like:
Operation type|Number of requests per second|Number of RU per request|RU/s needed
    Write Single Document|10,000|10|100,000
    -|-|-|-
    Top Query #1|700|100|70,000
    Top Query #2|200|100|20,000
    Top Query #3|100|100|10,000
    Total RU/s|||200,000 RU/s

# Evaluate data storage requirements
* Migrating existing transactional workloads - Use CosmosDB Capacity Calculator to help estimate your app's storage and throughput requirements and translate it to a cost estimate in terms of CosmosDB.
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/2ce0ab5e-b750-4719-aa49-e19f025b103e)

# Time-to-live (TTL)
* Set the length of time documents live in the db before being automatically purged. It's measured in seconds from the last modification and can be set at the container level with the ability to override on a per-item basis.
* The maximum TTL value is 2147483647.
* Configuring TTL on a container using the <ins>DefaultTimeToLive</ins> property: Does not exist(not automatically expired), -1(not expire by default), n(n seconds after last modified time).
* Examples
    Container.DefaultTimeToLive|Item.ttl|Expiration in seconds
    -|-|-
    1000|null|1000
    1000|-1|This item will never expire
    1000|2000|2000
    Container.DefaultTimeToLive|Item.ttl|Expiration in seconds
    null|null|This item will never expire
    null|-1|This item will never expire
    null|2000|TTL is disabled at the container level. This item will never expire.

# Move data into and out of Azure Cosmos DB for NoSQL
* <ins>Azure Data Factory</ins> is a native service to ETL across sinks and stores in an entirely serverless fashion.
* <ins>Apache Kafka</ins> used to stream events in a distributed manner. Kafka Connect is a tool within their suite to stream data between Kafka and other data systems.
* <ins>Azure Stream Analytics</ins> is a real-time event-processing engine designed to process fast streaming data from multiple sources simultaneously.
* <ins>With Azure Synapse Analytics and Azure Synapse Link for Azure Cosmos DB</ins>, you can create a cloud-native hybrid transactional and analytical processing (HTAP) to run analytics over your data in Azure Cosmos DB for NoSQL. This connection enables integration over your data pipeline on both ends of your data world, Azure Cosmos DB and Azure Synapse Analytics.

# Enable offline development
* <ins>The Azure Cosmos DB emulator</ins is a great tool for common Dev+Test workflows that developers may need to implement on their local machine.
    
* Handle connection errors - Built-in retry    
    
# Implement threading and parallelism
* While the SDK implements thread-safe types and some degrees of parallelism, there are best practices that you can implement in your application code to ensure that the SDK has the best performance it can possibly have in your workload.
* Avoid resource-related timeouts - occur due to high CPU or port utilization on client machines rather than a service-side issue. So scale-out.
* Use async/await in .NET -  series of Task-based features to asynchronously invoke SDK client methods. 
* Use built-in iterators instead of LINQ methods - LINQ methods such as ToList will eagerly and synchronously drain a query while blocking any other calls from executing. 
    1. For example, this invocation of ToList() will block all other calls and potentially retrieve a large set of data: container.GetItemLinqQueryable<T>().Where(i => i.categoryId == 2)   .ToList<T>();
    2. The SDK includes methods such as ToFeedIterator<T> that asynchronously retrieves the results of a query without blocking other calls. container.GetItemLinqQueryable<T>().Where(i => i.categoryId == 2).ToFeedIterator<T>();
    3. Configure max concurrency, parallelism, and buffered item count - When issuing a query from the SDK, the QueryRequestOptions includes a set of properties to tune a query's performance.
        1. Max item count - All query results are returned as "pages" of results. the number of items you would like to return in each "page". Default is 100. set -1 to set a dynamic page size.
        2. Max concurrency - the number of concurrent operations ran client side during parallel query execution. If set to 1, parallelism is effectively disabled. If set to -1, the SDK manages this setting. Ideally, you would set this value to the number of physical partitions for your container.
        3. Max buffered item count - the maximum number of items that are buffered client-side during a parallel query execution. If set to -1, the SDK manages this setting. The ideal value for this setting will largely depend on the characteristics of your client machine.

# transactions in the context of JavaScript SDK  
* Stored procedures are scoped to a single logical partition. You cannot execute a stored procedure that performs operations across logical partition key values.
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/e5f77814-9ed5-4daf-98d9-28933476e019)
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/f39966c5-3ef8-434d-9cad-ac0af2febb98)
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/ba77777a-f205-4e91-a526-c1206978ae3c)

# Create user-defined functions    
* UDFs can only be called from inside queries as they enhance and extend the SQL query language.    
* ``` mark
    A simple SQL query: 
        SELECT p.name, p.price FROM products p   
    return
        { "name": "Black Bib Shorts (Small)", "price": 80.00 }    
  ```  
* ``` 
    updated query with the udf function "addTax": 
        SELECT p.name, p.price, udf.addTax(p.price) AS priceWithTax FROM products p  
    return:
          {
            "name": "Black Bib Shorts (Small)",
            "price": 80.00,
            "priceWithTax": 92.00
          }
    
    function addTax(preTax) {
        return preTax * 1.15;
    }    
  ```  
    
# Add triggers to an operation    
* Triggers can inject business logic both before and after operations. 
* Triggers are resources stored within a container, and their code is written in JavaScript, much like stored procedures and user-defined functions.   
* Pre-trigger Example - before an operation and cannot have any input parameters. Adding label, if not exist
* ``` mark    
    {
      "id": "caab0e5e-c037-48a4-a760-140497d19452",
      "name": "Handlebar",
      "categoryId": "e89a34d2-47ee-4da8-bcf6-10f552604b79",
      "categoryName": "Accessories",
      "price": 50
    } 
    
    function addLabel() {
        var context = getContext();
        var request = context.getRequest();

        var pendingItem = request.getBody();

        if (!('label' in pendingItem))
            pendingItem['label'] = 'new';

        request.setBody(pendingItem);
    }
    
    {
      "id": "caab0e5e-c037-48a4-a760-140497d19452",
      "name": "Handlebar",
      "categoryId": "e89a34d2-47ee-4da8-bcf6-10f552604b79",
      "categoryName": "Accessories",
      "price": 50,
      "label": "new"
    }
  ```
* Post-trigger example-  run after an operation has completed and can have input parameters even though they are not required.  create a second item with a different materialized view of our data
* ``` mark
    {
      "sourceId": "caab0e5e-c037-48a4-a760-140497d19452",
      "categoryId": "e89a34d2-47ee-4da8-bcf6-10f552604b79",
      "displayName": "Handlebar [Accessories]",
    }
    
    function createView() {
        var context = getContext();
        var container = context.getCollection();
        var response = context.getResponse();

        var createdItem = response.getBody();

        var viewItem = {
            sourceId: createdItem.id,
            categoryId: createdItem.categoryId,
            displayName: `${createdItem.name} [${createdItem.categoryName}]`
        };

        var accepted = container.createDocument(
            container.getSelfLink(),
            viewItem,
            (error, newItem) => {
                if (error) throw error;
            }
        );
        if (!accepted) return;
    }
  ```  

# Understand Always Encrypted   
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/058badc6-c56d-421b-8133-990cf4231993)

# Azure Monitor  
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/9ee2cd71-97e9-4aea-be63-8f55cdf2e2f1)
* Cosmos DB monitors its server-side counters using:
    1. Azure Monitor to monitor metrics: Collects Cosmos DB metrics by default. Metrics are collected every minute. The default retention period is 30 days. 
    2. Azure Monitor to monitor diagnostic logs: Telemetries like events and traces are stored as logs. 
    3. The Azure Cosmos DB portal:  The default retention period for these metrics is seven days.
    4. The Cosmos DB NoSQL API SDKs to programmatically monitor the account

    
# Measure throughput
* Azure Monitor for CosmosDB provides the Total RUs metric that can be used to analyze the RUs consumed by the different CosmosDB operations. This metric can then be used to analyze those operations with the highest throughput.
* Monitoring this metric allows us to:
    1. Identify operations that are consuming more RUs than others.
    2. Identify operations that are taking more cumulative RUs in a given interval of time.
* By identifying the operations with higher throughput, we can for example:
    1. Determine if these operations are insert and upserts, their index definition can be reviewed for over or under indexing-specific fields. We can then determine if we should include or exclude paths in their indexing policy.
    2. Modify the query to use and index with a filter clause.
    3. Use partition keys that will minimize the fan out of a query into different partitions.
    4. If possible, evaluate if a smaller result set would meet the query needs.
* View the Total Request Unit metrics
* To view the Total Request Units metric, under Azure Monitor's Metrics:
    1. Select the Resource Type CosmosDB accounts and Apply in the scope dialog.
    2. Select the correct CosmosDB account from the drop-down list.
    3. Under Metrics, select Total Request Units and the type of aggregation you need.
    4. If needed, refine the Time range and Time granularity of the metric.
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/56a6a376-9281-44a4-861b-d3821043f88e)
* Filter the Total Request Units further - To better analyze the throughput, Using the Add filter and Apply splitting options will help us with those analyses. Azure Monitor allows us to filter further by specific CollectionName, DatabaseName, OperationType, Region, Status, and StatusCode. 
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/96e32dfa-bc18-4eed-b139-fc56bc4d5026)
    
# Observe rate-limiting events
* 429 status code error, a Request rate too large exception
* 3 main reasons why we get a 429 exception:
    1. Request rate is large.
    2. The request did not complete due to a high rate of metadata requests.
    3. The request did not complete due to a transient service error.
* Request rate is large, investigate - Cosmos DB account -> Under Insights->Request menu ->Total Request by Status Code charts -> search for occurrences of the 429 exception -> If needed, filter the charts by Time Range and Database
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/fc1d36ea-c2da-46e8-a96b-4e1c22172835)
    * the % of 429 exceptions > 5%, it's possible that the exceptions are caused by a hot partition.
    * To verify if the database access is coming across a hot partition -> under Insights->Throughput, review the Normalized RU Consumption (%) By PartitionKeyRangeID charts.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/a6157e41-9549-45f7-954d-39ee3830133e)
* Rate limiting on metadata requests - a high volume of the following metadata operations:
    1. Create, read, update, or delete a container or database
    2. List databases or containers in a Cosmos account
    3. Query the current provisioned throughput
* investigate -> under Insights->System, review the Metadata Requests That Exceeded Capacity (429s) charts.
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/62ae3fb0-32e0-4ced-bb27-0ef6d8eee7c2)
* Rate limiting due to transient service error (503) - reported in the Insights->Request Total Request by Status Code charts. These exceptions could indicate that the 429 exceptions are happening because of transient service errors.    
    
# Index usage
* The query engine evaluates query filters and then traverses the index of your container. 
* The query engine will automatically try to use the most efficient of the following three methods of evaluating filters:
Method|Description|RU implication
|-|-|-
Index seek|seek an exact match on a field’s value.|The RU charge is constant for the lookup. The RU charge for loading and returning items is linear based on the number of items.
Index scan|find all possible values for a field.|The RU charge is still constant for the lookup, with a slight increase over the index seek based on the cardinality of the indexed properties. The RU charge for loading and returning items is still linear based on the number of items returned.
Full scan|load the items, in their entirety, to the transactional store to evaluate the filters.|This type of scan does not use the index; however, the RU charge for loading items is based on the number of items in the entire container.
* Example
    ``` mark
            [
              {
                "id": "1",
                "name": "Touring-1000 Blue",
                "price": 675.55
              },
              {
                "id": "2",
                "name": "Mountain-400-W Silver",
                "price": 1215.40
              },
              {
                "id": "3",
                "name": "Road-200 Red",
                "price": 405.85
              }
            ]
    ```
* Each of these items could be visualized as a tree -same like 3rd also
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/b781736b-5f6b-42d7-8d77-2bb882a28318)
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/89895ce8-0cbf-474e-80fe-d38bab070023)    
* <ins>An inverted tree</ins> that includes all three items would have a root node that matches all three items. 
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/3dd021aa-33a9-4add-9889-ce78f97e0a7c)
* To traverse the tree, a SQL query: SELECT * FROM products p WHERE p.name = 'Touring-1000 Blue'
* The query engine could then traverse the tree in the following order:
    1. The engine will start at the root (all items are still potential matches).
    2. traverse the name node. Still, all items match.
    3. traverse the exact match, Only item #2 matches at this point
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/ac5d22e0-0114-4f55-a5b8-e2b0199c8f5a)
* This traversal is an example of the <ins>index seek</ins> lookup method in action.
    * The matching of an exact value is a <ins>flat charge in RU/s</ins> since it uses the index instead of searching in each item’s. If no match, no items will be returned in the result set. If multiple items, the query engine to return multiple items.
* Another example of an index seek: SELECT * FROM products p WHERE p.name IN ('Road-200 Red', 'Mountain-400-W Silver')
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/3dfaaf4d-1930-43f6-be31-6dba13a92d13)
*  complex index lookup method: SELECT * FROM products p WHERE p.price >= 500 AND p.price <= 1000
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/237b5836-f699-4d25-abc1-52b4a1ff5ec9)
                                                                                                
# Denormalize data in your model
* the product table from your relational database and model it for a NoSQL database. the many-to-many relationship your product table has with product tags.
* ![image](https://github.com/mkader/Developer-Tools/assets/3132680/8597837a-377a-4e2a-a93e-b7667ea274bb)
                                                                                         * Model the product entities
    1. application must display the product category name when you display a product.
    2. query for products by product tags within a product category as well. 
    3. 2 ways: store products in a product tags container, or embed your tags in the product container. 
    4. fewer tags per product than products per tag, so embed the product tags in the product table. There is a one-to-few relationship between each product and the tags, which makes a good case for embedding. 
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/3df12a8a-b5b3-496e-8153-7d8cc9a2dffe)
* Select a partition key - the operations (crete or edit) to be performed to decide on a partition key. 
    1. select a partition key for the product container. 
    2. As customers navigate by product category (filters products by categoryId)
    3. a single-partition query with all products by category.
    4. categoryId is a good partition key, retrieve all products in a category efficiently
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/0f437f9d-3dc9-4553-8fa9-641a5f6603aa)
    5. Currently, to display a product page for a category: Query the product container, the productCategory (name) container, using the first query, run a 3rd query on the productTag container to get each product tag name.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/005e8a25-8e2f-4b54-a8a6-36a9e0792fd5)

* Denormalize product entities
    1. Running all the preceding queries could work for you (not scalable). 
    2. in NoSQL, no joins between containers. 
    3. For NoSQL, reduce requests number can fetch data in as few requests as possible.
    4. The solution, is to denormalize data. Add more properties (category name and each tag in your tags array. Now retrieve all the data in a single request.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/de4f5e73-87fd-4159-852c-fd5ef8d8010b)
                                                                                         * Manage referential integrity by using change feed
    1. denormalizing data can drastically improve performance and lower RU cost by providing the data needed in a single request. 
    2. when data is denormalized like this, maintain <ins>referential integrity</ins> between the master data in the productCategory and productTag containers, and the product container.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/6e2c8684-20ff-4fed-932c-67a64bc8af84)
    3. CosmosDB <ins>change feed</ins> API that can manage referential integrity. Lives within every CosmosDB container. 
    4. insert or update, change feed streams these changes to an API that you can listen to. 
    5. When an event is triggered, you can use change feed to execute code that responds to the changed data.
    6. use change feed to listen to the productCategory, productTag and propagate changes to the product container.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/394b6964-141b-4cb2-9b91-6bbdc132aa74)
    
# Combine multiple entities in the same container
* Model sales order entities
    1. sales order detail makes a great candidate for embedding. This is because the items in the order are not unbounded and the data is always inserted and read together. So you'll embed sales order details as an array within your sales order entity. And you'll store your data in a new container, called salesOrder.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/3a9ecf70-0641-44ab-985b-821b7f4abb95)
    2. Choose partition key - always search for sales order by customer, customerId.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/cd5f6fca-c88d-4eb6-a8b7-c02af200e2c1)
     
* Identify optimization opportunities
    1. the salesOrder container shares the same partition key as the customer container.
    2. The customer container has a partition key of ID and salesOrder has a partition key of customerId. 
    3. When data share a partition key and have similar access patterns, they're candidates for being stored in the same container. 
    4. As a NoSQL database, CosmosDB is schema agnostic, so mixing entities with different schema is not only possible but, under these conditions, it's also another best practice. But to combine the data from these two containers, you'll need to make more changes to your schema.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/4a7a7528-962a-416a-a682-6ee8e3cf26a7)
    5. diagram showing the sales orders and customer models and containers going to a single customer container with both the sales order and customer documents stored in it.
    6. add a customerId property to each customer document. Customers will now have the same value for ID and customerId. 
    7. distinguish a sales order from a customer in the container.add a type property, value of customer and salesOrder for each entity.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/07ca81a8-1c66-44ac-8486-96216171ec3a)
    8. now store both the customer data and sales order data in your new customer container. Each customer is in its own logical partition and will have one customer item with all its sales orders. For your second operation here, you now have a query you can run to list all sales orders for a customer.

* Denormalize aggregates in the same container
    1. last operation - query top 10 customers by the number of sales orders. 
    2. In your current model, you first do a group by on each customer and sum for sales orders in your customer container. You then sort in descending order and take the top 10 results. 
    3. Even though customers and sales orders sit in the same container, this type of query is not something you can currently do.
    4. The solution here is to denormalize the aggregate value in a new property, salesOrderCount, in the customer document. 
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/deb562bf-6bf8-47a1-ab3a-bbe70fa422c0)
    5 Now, every time a customer creates a new sales order and a new sales order is inserted into your customer container, you need a way to update the customer document and increment the salesOrderCount property by one. 
    6. To do this by a transaction,  when the data sits within the same logical partition.
    7. Because customers and sales orders reside in the same logical partition, you can insert the new sales order and update the customer document within a transaction. 
    8. implement transactions: by using SPs or a feature called transactional batch
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/cfd73d31-7515-40f7-b889-1a342b781a17)                                                                 
    
# Finalize the data model
    1. transformed nine relational database tables into four containers for your NoSQL database. 
    2. customer container contains customer and sales order data. 
    3. The product container contains products and many-to-many product tags. 
    4. the other two are the productTag and productCategory containers.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/d892cc83-b22e-455b-9978-b38d347996b6)
    5. One final optimization - the productCategory and productTag containers share the same partition key? because they share this key, put both entities into the same container and give it a more generic name, such as productMeta.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/10371eac-991a-43b0-874e-b1295b5e9e99)
    6. queries to get all your product tags and product categories. This pattern works for any kind of master or reference data you need to maintain. 
    7. Because all of this data is in the same container, you can also use just a single host for Change Feed to maintain referential integrity across the entire database, rather than one for each individual container. 
    8. change feed for any entity can be routed by the type property when the new data is read by change feed.
    * ![image](https://github.com/mkader/Developer-Tools/assets/3132680/6c25391d-bd35-41aa-81a7-87e2a62c3c32)
